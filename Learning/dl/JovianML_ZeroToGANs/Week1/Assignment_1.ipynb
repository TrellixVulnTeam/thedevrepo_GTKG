{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1: `torch.randn`\n",
    "\n",
    "This function generates a tensor consisting of random  numbers sampled from a Standard Normal Distribution (mean=0, std=1). The function accepts the following arguments, let's look at them one by one:\n",
    "\n",
    "- `size`: The size of the resultant tensor, usually defined as a `list` or a `tuple`\n",
    "- `out`: This optional argument lets you specify where the resultant tensor has to be stored, this must be a Torch tensor (otherwise, it either throws an error or never assigns the value). By default, the argument is set to `None` and in which case, the object returned will have to be explicitly assigned to a variable\n",
    "- `dtype`: The type for the resultant tensor can be specified here (optionally). By default, it is set to `None` and in which case, the global default tensor type would be set\n",
    "- `layout`: Optional argument to specify the desired layout for the resultant tensor. By default, this arg is set to `torch.strided`. More on `torch.strided` in the below section\n",
    "- `device`: Optional argument to specify the desired device to attach the tensor instance to. Defaults to `None` and can be specified using `torch.device()` for CPU/GPU\n",
    "- `requires_grad`: This is a boolean argument set to `False` by default. Specifies if the different operations on this tensor have to be recorded. If `True`, all operations would be recorded for Torch to calculate the gradient (using autograd). This is relevant during the backward pass / back propagation in a Neural Net training.\n",
    "\n",
    "**Possible application**: This is normally used for randomly generating tensors to initialize parameters or even inputs in certain cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9656, -0.5504,  0.6922,  0.3452],\n",
       "        [-0.7286, -0.4964,  0.1163,  0.1918]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working example\n",
    "\n",
    "temp = torch.Tensor()\n",
    "torch.randn(size=[2,4], out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9656, -0.5504,  0.6922,  0.3452],\n",
       "        [-0.7286, -0.4964,  0.1163,  0.1918]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `temp` was initialized as a tensor and hence it'd receive the random tensor by assignment. However, if the variable is of incompatible type the operation would throw an error. Particularly, for `None` type, the assignment never happens although no error is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7126,  0.2565, -0.8528, -0.4314],\n",
       "        [ 1.2556,  0.7384,  0.9697,  0.9443]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad example\n",
    "\n",
    "some_variable = None\n",
    "torch.randn(size=[2,4], out=some_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# bad example output\n",
    "# the variable is never set\n",
    "print(some_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on strides**: \n",
    "\n",
    "`torch.strided` is responsible for deciding the memory layout of tensors.\n",
    "\n",
    "To explain in simple words, we need to take a step back and look at tensors. Clearly, a tensor is a multi-dimensional representation of values (numeric, in this context) and these values require to be stored in some location in memory. \n",
    "But how are they represented in memory ? The answer is that there'd be a memory layout that's associate with the tensor. But who takes care of them ? Strides. \n",
    "\n",
    "Strides would provide a list of integers that help in calculating the jumps in memory locations to access the different values of a tensor (please bear in mind that the tensor values may not be allocated contiguous memory locations). \n",
    "\n",
    "Now, coming back to PyTorch, the default memory layout representation is taken care by `torch.strided`, which is a dense tensor representation. However, there's a sparse representation (experimental) support which is available too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 2: `torch.reshape` \n",
    "\n",
    "The `reshape` function takes in an input tensor and alters its shape as specified.\n",
    "Looking at the arguments,\n",
    "\n",
    "- `input`: Should be a valid tensor. The reshape would either provide a \"view\" of the tensor (no copy) if possible, otherwise returns a copy of the reshaped tensor\n",
    "- `shape`: The new shape to which the input tensor has to be changed. This has to be a valid shape, otherwise the function would throw an error. A valid shape would stay true to its dimension - meaning it should be possible to arrange the number of elements properly (see the bad example provided below)\n",
    "\n",
    "**Possible application**: Reshape is a very handy functionality used during tensor multiplication operation that may arise while creating neural nets from scratch, calculating back-prop errors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9656, -0.5504],\n",
       "        [ 0.6922,  0.3452],\n",
       "        [-0.7286, -0.4964],\n",
       "        [ 0.1163,  0.1918]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working example\n",
    "torch.reshape(input=temp, shape=(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 3]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2972a241791c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# bad example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 3]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "# bad example\n",
    "torch.reshape(input=temp, shape=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 3: `torch.argmax`\n",
    "\n",
    "This function returns the index of the maximum value among all the elements in the tensor.\n",
    "\n",
    "Following are the arguments allowed:\n",
    "- `input`: The input tensor is to be provided here\n",
    "- `dim`: This optional argument expects an integer input that specifies the dimension along which the max is to be found out. Depending on the input dimension, the input tensor would be reduced to a resultant form. See below example for more clarity. If the argument is `None`, the input is assumed as a flattened tensor\n",
    "- `keepdim`: This is a boolean flag that decides whether to retain the tensor dimension. This argument would be ignored if the `dim` argument is `None`. \n",
    "\n",
    "Note that for a given tensor, there's also an attribute function of `argmax`.\n",
    "\n",
    "**Possible application**: This is a reduce operation that's useful while we look at prediction probabilities in a multi-class classification problem, where usually the output would be a prediction vector. This would let us zero-in on the relevant part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp:\n",
      "tensor([[-0.9656, -0.5504,  0.6922,  0.3452],\n",
      "        [-0.7286, -0.4964,  0.1163,  0.1918]])\n",
      "\n",
      "torch.argmax(temp) is: 2\n",
      "\n",
      "torch.argmax(temp, dim=1)\n",
      "gives the argmax across the first dimension,i.e row1 and row2: \n",
      "tensor([2, 3])\n",
      "\n",
      "torch.argmax(temp, dim=1, keepdim=True)\n",
      "gives the argmax across the first dimension (retaining the input dimensions): \n",
      "tensor([[2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# working example\n",
    "\n",
    "print(\"temp:\\n{}\".format(temp))\n",
    "\n",
    "print(\"\\ntorch.argmax(temp) is: {}\".format(torch.argmax(temp).item()))\n",
    "\n",
    "print(\"\\ntorch.argmax(temp, dim=1)\\ngives the argmax across the first dimension,i.e row1 and row2: \\n{}\".\n",
    "      format(torch.argmax(temp, dim=1)))\n",
    "\n",
    "print(\n",
    "    \"\\ntorch.argmax(temp, dim=1, keepdim=True)\\ngives the argmax across the first dimension (retaining the input dimensions): \\n{}\".\n",
    "      format(torch.argmax(temp, dim=1, keepdim=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**How does the `dim` argument work ?**\n",
    "A tensor is essentially a multi-dimensional array. Each dimension represents an axis here.\n",
    "\n",
    "For simplicity, let's consider the 2D Tensor as an example,\n",
    "\n",
    "            Column axis (axis=1)\n",
    "         ------------------------------------->\n",
    "         [[-1.1083,  1.7481,  0.4163,  1.3754],\n",
    "         [-0.5549,  0.3910,  0.8286, -1.1974]]\n",
    "        \n",
    "\n",
    "Here, it consists of only 2 axes - the row axis and column axis. The row axis let's us traverse along the rows (vertical) in a tensor, while the column axis takes us along the columns (horizontal).\n",
    "\n",
    "To summarize,\n",
    "\n",
    "`axis=0` represents the row axis.\n",
    "\n",
    "`axis=1` represents the column axis.\n",
    "\n",
    "Furthermore, as the dimensions of the tensor increases (3D Tensors, or higher) they'll have additional axis/es for traversal.\n",
    "\n",
    "Below is a bad example, where the axis dimensions are incorrectly defined while using `argmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-674b610a4d6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m print(\"\\ntorch.argmax(temp, dim=1)\\ngives the argmax across the second dimension,i.e row1 and row2: \\n{}\".\n\u001b[1;32m----> 4\u001b[1;33m       format(torch.argmax(temp, dim=2)))\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# bad example\n",
    "\n",
    "print(\"\\ntorch.argmax(temp, dim=1)\\ngives the argmax across the second dimension,i.e row1 and row2: \\n{}\".\n",
    "      format(torch.argmax(temp, dim=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar functions:\n",
    "\n",
    "- `torch.argmin`\n",
    "- `torch.min` and `torch.max`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 4: `torch.abs`\n",
    "\n",
    "The function returns the absolute valuefor a given input tensor. This accepts an input tensor as an argument and optionally an output tensor to which the result would go.\n",
    "\n",
    "About the arguments,\n",
    "\n",
    "- `input`: The input tensor\n",
    "- `out`: The tensor to which the output would be assigned.\n",
    "\n",
    "Here, one should ensure that the `out` argument receives a valid/correctly initialized tensor. Otherwise, the returned result won't be assigned.\n",
    "\n",
    "**Potential application:** During calculation of evaluation metrics like certain loss functions during training, there'd be operations on top of the absolute values of the vectors/tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9656, -0.5504,  0.6922,  0.3452],\n",
      "        [-0.7286, -0.4964,  0.1163,  0.1918]])\n",
      "\n",
      "\n",
      "tensor([[0.9656, 0.5504, 0.6922, 0.3452],\n",
      "        [0.7286, 0.4964, 0.1163, 0.1918]])\n"
     ]
    }
   ],
   "source": [
    "# working example\n",
    "abs_temp = torch.Tensor()\n",
    "\n",
    "print(temp)\n",
    "\n",
    "torch.abs(input=temp, out=abs_temp)\n",
    "print(\"\\n\")\n",
    "print(abs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9656, -0.5504,  0.6922,  0.3452],\n",
      "        [-0.7286, -0.4964,  0.1163,  0.1918]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# bad example\n",
    "\n",
    "some_tensor = None\n",
    "\n",
    "print(temp)\n",
    "\n",
    "torch.abs(input=temp, out=some_tensor)\n",
    "\n",
    "print(some_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 5: `torch.add`\n",
    "\n",
    "This function adds a scalar to each element of a given input tensor and returns the new tensor as output.\n",
    "Here, the type of scalar should be compatible with that of the intput tensor.\n",
    "\n",
    "Following are the arguments taken by `torch.add`:\n",
    "\n",
    "- `input`: The input tensor\n",
    "- `other`: The scalar that would be added to the given tensor\n",
    "\n",
    "**Potential applications**: If one is trying to implement certain optimization techniques from scratch (like, Gradient Descent), there would steps involving scalar updates to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.9656, 7.5504, 7.6922, 7.3452],\n",
       "        [7.7286, 7.4964, 7.1163, 7.1918]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working example\n",
    "\n",
    "bad = torch.Tensor()\n",
    "scalar_float = 7.\n",
    "\n",
    "torch.add(abs_temp,other=scalar_float, out=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an incorrect operation where `other` argument doesn't receive a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f9e5acb6e29a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# bad example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# bad example\n",
    "\n",
    "torch.add(abs_temp, other=bad, out=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
